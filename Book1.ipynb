{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">APIrates - Predicting New York City Home Sales</h1> \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jkelleher17/apirates/gh-pages/images/new-york-city.png\" width=600 height=300>\n",
    "\n",
    "\n",
    "<a style=\"float: right;\" href=\"http://www.tripadvisor.com/Tourism-g60763-New_York_City_New_York-Vacations.html\">Source</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Collection and Cleaning\n",
    "This IPython notebook handles all the collection and cleaning of the data to prepare for statistical analysis. It gives an introduction to our entire project - the hypotheses, goals and methodology. There are many sources of data that we used. We gather information from the NY Department of Finance, NY Department of Planning, American Community Survey, US Census, and others. This gives us a wealth of information to augment the primary data source - all the homes sold in New York City. But the data is quite dirty - there are missing values, ommitted columns and clearly inaccurate data. We address these issues here during the cleaning. This ensures that the data is usable and ready for statistical analysis.  By the end of the notebook, all features are assembled in a single dataframe. This dataframe is then used in subsequent notebooks for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Table of Contents\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Data Collection](#2)\n",
    "    * [Scrape the Home Sale Data](#2.1)\n",
    "    * [Same Data from CSV Files](#2.2)\n",
    "    * [Visualize the Data](#2.4)\n",
    "3. [Preliminary Data Cleaning](#3) \n",
    "4. [Feature Addition](#4)\n",
    "    * [Augument with Local Data](#4.1)\n",
    "    * [Add Demographic and Education Information](#4.2)\n",
    "    * [Visualize the Demographics](#4.3)\n",
    "    * [2010 Demographic Data](#4.4)\n",
    "    * [Education Data](#4.5)\n",
    "    * [Infomation about the Adjacent Zip Codes](#4.6)\n",
    "    * [Augument with Additional Information](#4.7)\n",
    "5. [A Little Housing Cleaning before Analysis](#5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Introduction\n",
    "<a id = '1'></a>\n",
    "###Project Overiew\n",
    "Housing prices are notoriously difficult to predict. Most models use features intrinstic to the individual houses (number of rooms, baths, square footage, distance to school) to predict the sale price. This strategy often says very little about how external features factor in  and how the housing market, in general, operates to influence home prices.\n",
    "\n",
    "We begin our investigation with the hypothesis that home sales in similar locations will move in similar directions. That is; external factor about where a school is located (such as education, crime, hospitals, etc.) as well as nearby homes sales influence the value of a home.\n",
    "\n",
    "To this end, we propose a model that predicts to predict the median price per zip code in New York City. \n",
    "\n",
    "\n",
    "###Background and Motivation\n",
    "\n",
    "\n",
    "###Initial Questions\n",
    "\n",
    "\n",
    "###Overview Of Our Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#2. Data Collection\n",
    "<a id = '2'></a>\n",
    "\n",
    "Home sales were collected from data provided by the NY Department of Finance. Since 2003, the Department has publically released all home sold, sorted by borough and year. This information is found here, http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page. We scrape this site for the excel files pertaining for annual home sale data the from 2007 - 2014.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Scrape the Home Sale Data\n",
    "<a id = '2.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "excels = soup.findAll('a',href = re.compile(r'.*.xls'))#get all excel files \n",
    "links = [i.get('href') for i in excels][13:53] #Specific links we need; not great general scraping practice,\n",
    "                                                #but we are only working with a single page \n",
    "allFiles = ['http://www1.nyc.gov/' + str(link) for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split into two bunchs, based on their different formats\n",
    "group_1 = allFiles[:20]\n",
    "group_2 = allFiles[20:]\n",
    "\n",
    "list_ = [] # create list of dataframes\n",
    "\n",
    "#excels files for 2011 - 2014\n",
    "for f in group_1:\n",
    "    df = pd.read_excel(f, skiprows = 2, header = 2)\n",
    "    df.columns = [i.strip('\\n') for i in df.columns]\n",
    "    \n",
    "    #some years have tabs at end of column names\n",
    "    df.columns = [i.strip('\\n') for i in df.columns]\n",
    "    \n",
    "    \n",
    "    #Make sure SALE DATE column is a Date Type\n",
    "    df['SALE DATE'] = pd.to_datetime(df['SALE DATE'])\n",
    "    \n",
    "    #create year column\n",
    "    df['YEAR'] = df['SALE DATE'].apply(lambda r: r.year)\n",
    "    list_.append(df)\n",
    "    \n",
    "    \n",
    "#excel files for 2007 - 2010\n",
    "for f in group_2:\n",
    "    df = pd.read_excel(f, skiprows = 1, header = 2) #this is the different line from above\n",
    "    df.columns = [i.strip('\\n') for i in df.columns]\n",
    "    \n",
    "    df.columns = [i.strip('\\n') for i in df.columns]\n",
    "    \n",
    "    df['SALE DATE'] = pd.to_datetime(df['SALE DATE'])\n",
    "    \n",
    "    df['YEAR'] = df['SALE DATE'].apply(lambda r: r.year)\n",
    "    list_.append(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the DataFrames into one single Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawdf = pd.concat(list_,)\n",
    "rawdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Same Data from CSV Files\n",
    "<a id = '2.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote these `Excel` files onto disk as `CSV` files. This is the same data as above. We did this in case the files broke, or there was some other issue. If there is some issue with the above links, uncommenting the lines below will produce the same df.\n",
    "\n",
    "If the code above runs normally, as it should; there is no need to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "#all csv files saved in file AllYears\n",
    "\"\"\"\n",
    "path = os.getcwd() + '/AllYears/'\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    \n",
    "    #some years have tabs at end of column names\n",
    "    df.columns = [i.strip('\\n') for i in df.columns]\n",
    "    \n",
    "    \n",
    "    #Make sure SALE DATE column is a Date Type\n",
    "    df['SALE DATE'] = pd.to_datetime(df['SALE DATE'])\n",
    "    \n",
    "    #create year column\n",
    "    df['YEAR'] = df['SALE DATE'].apply(lambda r: r.year)\n",
    "    list_.append(df)\n",
    "rawdf = pd.concat(list_,)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualizing Which Homes Have Been Sold\n",
    "<a id = '2.4'></a>\n",
    "We first try see the location where each homes sales took place. For visualization purposes, we are only doing Manhattan: the dataset contains the home sales for each borough.\n",
    "\n",
    "In order to map each home sale, we use a ShapeFile provided by the NYC Department of Planning <a href=\"http://www.nyc.gov/html/dcp/html/bytes/dwn_pluto_mappluto.shtml#mappluto\">(link here)</a>. ShapeFile is a geospaial vector data formet, popular for handling geographic information.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shapefile #Deals with shapefiles\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myshp = open(\"Manhattan/MNMapPLUTO.shp\", \"rb\")\n",
    "mydbf = open(\"Manhattan/MNMapPLUTO.dbf\", \"rb\")\n",
    "\n",
    "sf = shapefile.Reader(shp=myshp, dbf=mydbf)\n",
    "shape_recs = sf.shapeRecords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate Manhattan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MN = rawdf[rawdf['BOROUGH'] == 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#This code used Cloud Crays's as an example. His work is found here https://gist.github.com/CloudCray/8711607.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "color = sns.hls_palette(8, l=.3, s=.8)[0]\n",
    "         \n",
    "for rec in shape_recs:\n",
    "    points = rec.shape.points\n",
    "    d = rec.record\n",
    "    c = '#B3B3B3' #gray color \n",
    "    i,k = d[1:3] #this information has the unique (Block, Lot) identifer for each location\n",
    "    \n",
    "    if not MN[(MN['BLOCK'] == i)&(MN['LOT'] == k)].empty: #check if this house is in the dataframe\n",
    "        c = color #red color \n",
    "    \n",
    "    patch = patches.Polygon(points,True,color=c) #plot the polygon (mini representation of the house)\n",
    "    ax.add_patch(patch)\n",
    "    \n",
    "    \n",
    "    \n",
    "ax.set_title('House Sales in Manhatten, 2007-2014')\n",
    "\n",
    "patch = patches.Circle((1, 1), radius = 1000, color= color, label='Lot sold between 2007-2014' )\n",
    "\n",
    "\n",
    "plt.legend(handles=[patch], bbox_to_anchor=(.6,0.25), loc='lower left', borderaxespad=0.)\n",
    "ax.autoscale()\n",
    "plt.axis('off')\n",
    "#plt.savefig('MH.png', transparent=True)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Preliminary Data Cleaning\n",
    "<a id = '3'></a>\n",
    "This dataset badly needs cleaning.\n",
    "\n",
    "We first keep only potential relevent columns and  homes that sold for over \\$1000. There are a number of $0 Sales. According to the Finance Department <a href=\"http://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf\">(link)</a>, \n",
    "\n",
    ">A \\$0 sale indicates that there was a transfer of ownership without a cash consideration.\n",
    ">'There can be a number of reasons for a $0 sale including transfers of ownership from\n",
    ">parents to children. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep =(\n",
    "[u'BOROUGH', u'NEIGHBORHOOD', u'BUILDING CLASS CATEGORY',\n",
    " u'TAX CLASS AT PRESENT', u'BLOCK', u'LOT',\n",
    "       u'BUILDING CLASS AT PRESENT', u'ADDRESS',\n",
    "       u'ZIP CODE', u'RESIDENTIAL UNITS', u'COMMERCIAL UNITS', u'TOTAL UNITS',\n",
    "       u'LAND SQUARE FEET', u'GROSS SQUARE FEET', u'YEAR BUILT',\n",
    "       u'TAX CLASS AT TIME OF SALE', u'BUILDING CLASS AT TIME OF SALE',\n",
    "       u'SALE PRICE', u'SALE DATE', 'YEAR'])\n",
    "\n",
    "df = rawdf[keep].reset_index(drop=True)\n",
    "df = df[df['SALE PRICE'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we deal with the issue of missing data. We filter out home sales where the `GROSS SQUARE FEET` is ommitted or impractially low. This allows us to then calculate the price per square foot. This will later be the variable that we attempt to predict using the dataset and other external data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['GROSS SQUARE FEET'] > 500]\n",
    "df['price_ft'] = df['SALE PRICE'] / df['GROSS SQUARE FEET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now delete homes that are not residential properities. The 21 `Building Class Category` corresponds to \"office buildings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df[(df['BUILDING CLASS CATEGORY'].apply(lambda r: int(r[:2]) != 21))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some 'homes' that snuck into the data set. The most expensive 'home' (it's actually an entire skyscraper) sold for over $3 billion in 2014. We delete it along with other sales that likely not residential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['SALE PRICE'] == df['SALE PRICE'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = df2[df2['SALE PRICE'] < 500000000]\n",
    "\n",
    "#Also delete duplicates\n",
    "sorta = df3.sort(['SALE PRICE'], ascending = False)\n",
    "updated_df = sorta.drop_duplicates(['BOROUGH','BLOCK','LOT','YEAR']).reset_index(drop = True)\n",
    "updated_df.sort(['ZIP CODE'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now keep only homes in zip codes where there has been a home sold each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Num_Years = 8\n",
    "EveryYear = (updated_df.groupby('ZIP CODE')['YEAR'].nunique() == Num_Years)\n",
    "zips = EveryYear[EveryYear == True].index\n",
    "\n",
    "\n",
    "threshold = 100\n",
    "Enough_Sales = (updated_df.groupby('ZIP CODE').count() > threshold)\n",
    "\n",
    "zips2 = Enough_Sales[Enough_Sales['BLOCK'] == True].index\n",
    "print len(zips), len(zips2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Find intersection of these lists\n",
    "keep_zips = np.intersect1d(zips,zips2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_df = updated_df[updated_df['ZIP CODE'].isin(keep_zips)].reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Feature Preparation and Addition\n",
    "<a id = '4'></a>\n",
    "Now that we have a relatively clean DataFrame, we augement it with outside data sources. We bring in time-seris information at the Zip Code level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we calculate the logs of the Sale Price and the Price per Square Feet. Both these distributions are heavily right-skewed, as there exists some very expensive homes that do not follow the same distribution. ***ADD SOME GRAPHS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['Log_Sale_Price'] = np.log(clean_df['SALE PRICE'].values)\n",
    "clean_df['Log_Price_Sqft'] = np.log(clean_df['price_ft'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these log values, we add historical data about each zip code. The below code works as follows: it groups the data by Zip Code and Year; and then finds the mean, standard deviation, count, and median of the `Log_Price_Sqft`. It then creates new columns in the DataFrame for 1-year ago, 2-years ago, and 3-years ago.\n",
    "\n",
    "Remember that we have data back until 2007, but we are only going to train and test on data from 2011 onwards. This way we can have values 3 years back for the relevent years. (We explain later why we omit 2010 data). There will be `NaN`s for 2007-2009 for these columns, but this will not matter during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = clean_df.groupby(['ZIP CODE', 'YEAR'])['Log_Price_Sqft']\n",
    "means = x.mean().to_dict()\n",
    "stds = x.std().to_dict()\n",
    "counts = x.count().to_dict()\n",
    "medians = x.median().to_dict()\n",
    "\n",
    "for i in xrange(1,4):\n",
    "    clean_df[str(i)+'-mean_log_Pft'] = (clean_df[clean_df.YEAR > 2009].\n",
    "                           apply(lambda r: means[(r['ZIP CODE'],r['YEAR']-i)], axis = 1)\n",
    "                           )\n",
    "    clean_df[str(i)+'-std_log_Pft'] = (clean_df[clean_df.YEAR >2009].\n",
    "                           apply(lambda r: stds[(r['ZIP CODE'],r['YEAR']-i)], axis = 1)\n",
    "                           )\n",
    "    clean_df[str(i)+'-count'] = (clean_df[clean_df.YEAR > 2009].\n",
    "                           apply(lambda r: counts[(r['ZIP CODE'],r['YEAR']-i)], axis = 1)\n",
    "                           )\n",
    "    clean_df[str(i) + '-median_log_Pft'] =(clean_df[clean_df.YEAR > 2009].\n",
    "                           apply(lambda r: medians[(r['ZIP CODE'],r['YEAR']-i)], axis = 1)\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Add Local Data\n",
    "<a id = '4.1'></a>\n",
    "\n",
    "We next add in data about each zip code. First, we input information about hospitals in each zip code. The thinking here is that locations with many treatment centers for 'bad' diseases and alignments says something about home sales price. This dataset was found through <a href = https://nycopendata.socrata.com/> NY OPEN DATA </a>. It gives the location of health facilities in each borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "health = pd.read_csv('health.csv')\n",
    "hospitals = {}\n",
    "bad_hosps = {}\n",
    "for row in health.iterrows():\n",
    "    if row[1]['primary_specialty'] == 'Addiction' or \\\n",
    "        row[1]['primary_specialty'] == 'HIV' or \\\n",
    "        row[1]['primary_specialty'] == 'Health Care for the Homeless':\n",
    "        \n",
    "        if row[1]['site_zip'] in bad_hosps:\n",
    "            bad_hosps[row[1]['site_zip']] += 1\n",
    "        else:\n",
    "            bad_hosps[row[1]['site_zip']] = 1\n",
    "    \n",
    "    if row[1]['site_zip'] in hospitals:\n",
    "        hospitals[row[1]['site_zip']] += 1\n",
    "    else:\n",
    "        hospitals[row[1]['site_zip']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the similiar vein, we also add information about local eateries from NY OPEN DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caf = pd.read_csv('cafes.csv')\n",
    "caf_dict = {}\n",
    "for row in caf.iterrows():\n",
    "    if row[1]['address_zip_code'] in caf_dict:\n",
    "        caf_dict[str(row[1]['address_zip_code'])] += 1\n",
    "    else:\n",
    "        caf_dict[str(row[1]['address_zip_code'])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caf_df = []\n",
    "hosp_df = []\n",
    "bad_hosp_df = []\n",
    "for row in clean_df.iterrows():\n",
    "    zc = str(row[1]['ZIP CODE'])\n",
    "    \n",
    "    if zc in hospitals:\n",
    "        hosp_df.append(hospitals[zc])\n",
    "    else:\n",
    "        hosp_df.append(0)\n",
    "        \n",
    "    if zc in bad_hosps:\n",
    "        bad_hosp_df.append(bad_hosps[zc])\n",
    "    else:\n",
    "        bad_hosp_df.append(0)\n",
    "        \n",
    "    if zc in caf_dict:\n",
    "        caf_df.append(caf_dict[zc])\n",
    "    else:\n",
    "        caf_df.append(0)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_df[\"Hospitals\"] = hosp_df\n",
    "clean_df['Bad Hospitals'] = bad_hosp_df\n",
    "clean_df['Street Cafes'] = caf_df\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Add Demographic and Education Data\n",
    "<a id = '4.2'></a>\n",
    "\n",
    "We now add demographic and education data for each zip code, and how it has changed over the years. As there is no single yearly census, this requires stiching together different data sources. For 2011-2014, we use data from the American Community Survery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip_codes = clean_df['ZIP CODE'].unique()\n",
    "zc = {}\n",
    "for elt in zip_codes:\n",
    "    zc[elt] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the ACS Files are stored in the folder `acs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dem_path = os.getcwd() + '/acs/'\n",
    "demFiles = glob.glob(dem_path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = {}\n",
    "for y in range(2011,2015):\n",
    "    year[y] = {}\n",
    "\n",
    "ctg = ['Population','White','Black','Asian','Hispanic']\n",
    "\n",
    "for f in demFiles:\n",
    "    with open(f, 'rU') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        print(\"File: \"+f)\n",
    "        if f[-8] == 'r':\n",
    "            race_df = {}\n",
    "            for c in ctg:\n",
    "                race_df[c] = {}\n",
    "            \n",
    "            for _ in range(0,3):\n",
    "                next(reader)\n",
    "                \n",
    "            for row in reader:\n",
    "                items = row[1]\n",
    "                lst = items.split(',')\n",
    "\n",
    "                if int(lst[0]) in zc.keys():\n",
    "                    \n",
    "                    race_df['Population'][int(lst[0])] = int(lst[1])\n",
    "                    try: \n",
    "                        w = int(lst[3])\n",
    "                    except:\n",
    "                        if lst[3] == '-':\n",
    "                            w = 0.\n",
    "                        else:\n",
    "                            w = float(lst[3])\n",
    "                    race_df['White'][int(lst[0])] = w\n",
    "                    \n",
    "                    try: \n",
    "                        b = int(lst[5])\n",
    "                    except:\n",
    "                        if lst[5] == '-':\n",
    "                            b = 0.\n",
    "                        else:\n",
    "                            b = float(lst[5])\n",
    "                    race_df['Black'][int(lst[0])] = b\n",
    "                    \n",
    "                    try:\n",
    "                        a = int(lst[9])\n",
    "                    except:\n",
    "                        if lst[9] == '-':\n",
    "                            a = 0.\n",
    "                        else:\n",
    "                            a = float(lst[9])\n",
    "                    race_df['Asian'][int(lst[0])] = a  \n",
    "\n",
    "                    try:\n",
    "                        h = int(lst[13])\n",
    "                    except:\n",
    "                        if lst[13] == '-':\n",
    "                            h = 0.\n",
    "                        else:\n",
    "                            h = float(lst[13])\n",
    "                    race_df['Hispanic'][int(lst[0])] = h  \n",
    "            \n",
    "            curr_y = int(f[-12:-8])\n",
    "            print(curr_y)\n",
    "            year[curr_y]['r'] = race_df\n",
    "        \n",
    "        elif f[-8] == 'g':\n",
    "            grad_df = {}\n",
    "            grad_df['High School Graduation Rate'] = {}\n",
    "            grad_df['College Graduation Rate'] = {}\n",
    "            \n",
    "            for _ in range(0,3):\n",
    "                next(reader)\n",
    "                \n",
    "            for row in reader:\n",
    "                items = row[1]\n",
    "                lst = items.split(',')\n",
    "\n",
    "                if int(lst[0]) in zc.keys():\n",
    "                    try:\n",
    "                        grad_df['High School Graduation Rate'][int(lst[0])]= float(lst[81])\n",
    "                    except Exception:\n",
    "                        grad_df['High School Graduation Rate'][int(lst[0])]= 0.\n",
    "                    try:\n",
    "                        grad_df['College Graduation Rate'][int(lst[0])]= float(lst[85])\n",
    "                    except Exception:\n",
    "                        grad_df['College Graduation Rate'][int(lst[0])]= 0.\n",
    "\n",
    "            curr_y = int(f[-12:-8])\n",
    "            year[curr_y]['g'] = grad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ = ['High School Graduation Rate', 'College Graduation Rate','Population', 'White', 'Black', 'Asian', 'Hispanic']\n",
    "new_dict = {}\n",
    "for n in new_:\n",
    "    new_dict[n] = []\n",
    "\n",
    "for row in clean_df.iterrows():\n",
    "\n",
    "    z_c = row[1]['ZIP CODE'] #get the zip code\n",
    "    yr = row[1]['YEAR'] #get the year\n",
    "    \n",
    "    if yr < 2011:\n",
    "        for k in new_dict.keys():\n",
    "            new_dict[k].append(0)\n",
    "        continue\n",
    "    \n",
    "    curr_yr = year[yr]\n",
    "    \n",
    "    grad_dict = curr_yr['g']\n",
    "    for grad in grad_dict.keys():\n",
    "        e = grad_dict[grad]\n",
    "        new_dict[grad].append(e[int(z_c)])\n",
    "        \n",
    "    race_dict = curr_yr['r']\n",
    "    for race in race_dict.keys():\n",
    "        r = race_dict[race]\n",
    "        new_dict[race].append(r[int(z_c)])\n",
    "\n",
    "for k, v in new_dict.items():\n",
    "    print k\n",
    "    clean_df[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mins = ['Asian', 'Black', 'Hispanic', 'White']\n",
    "\n",
    "for elt in mins:\n",
    "    clean_df[elt] = (clean_df[elt])/clean_df['Population']\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualize the Demographics\n",
    "<a id = '4.3'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myshp = open(\"shps/nyc-zip-code-tabulation-areas-polygons.shp\", \"rb\")\n",
    "mydbf = open(\"shps/nyc-zip-code-tabulation-areas-polygons.dbf\", \"rb\")\n",
    "sf = shapefile.Reader(shp=myshp, dbf=mydbf) #Create a shapefile reader\n",
    "shape_recs = sf.shapeRecords() #Information about each polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function `create_dem_graph` that takes a year and racial demographic (White, Black, Hispanic, Asian). It produces a map that depicts the percentage makeup of each zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dem_graph(year,cat, clean_df = clean_df):\n",
    "    d = clean_df[clean_df['YEAR'] == year]\n",
    "    tups  = d[['ZIP CODE',cat]].apply(lambda r: (r[0],r[1]), axis = 1).values\n",
    "    tups = dict(tups) #create dictionary of zip code -> percentage\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    color = 'r'\n",
    "    missing = '#EEEEEE'\n",
    "    for rec in shape_recs: #for each 'polygon'\n",
    "        points = rec.shape.points\n",
    "        d = rec.record #the .dbf file has information that accompanies each polygon. we extract the zip code \n",
    "        zip_code = float(d[-1])\n",
    "        c = missing\n",
    "        alpha = .5\n",
    "        if zip_code in tups.keys(): #assign correct color\n",
    "            alpha = tups[zip_code]\n",
    "            c = color\n",
    "        if alpha < .05:\n",
    "            alpha = .05\n",
    "\n",
    "\n",
    "        patch = patches.Polygon(points,True,color=c, alpha = alpha, ec = 'black', lw = 1) #plot each polygon\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "\n",
    "\n",
    "    #Create Legend    \n",
    "    percs = zip(['5% or less', '25%', '50%', '75%' , '100%'],[.05,.25,.50,.75,1])\n",
    "    lst = []\n",
    "    for label,alpha in percs[::-1]:\n",
    "        patch = patches.Patch(color= color, label=label, alpha = alpha)\n",
    "        lst.append(patch)\n",
    "\n",
    "    lst.append(patches.Patch(color = missing, label = 'No data'))\n",
    "\n",
    "    plt.legend(handles=lst, bbox_to_anchor=(.9,0.25), loc='lower left', borderaxespad=0., fontsize = 20, title = 'Percentage of %s Population' %(cat))\n",
    "\n",
    "\n",
    "    ax.set_title('%s Percentage, by Zip Code, in %i' %(cat,year))\n",
    "    ax.autoscale()\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_dem_graph(2014,'Black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_dem_graph(2014,'Hispanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_dem_graph(2011,'Asian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_dem_graph(2013,'White')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2010 Demographic Data\n",
    "<a id = '4.4'></a>\n",
    "\n",
    "\n",
    "This information comes from the U.S. Census Bureau, 2010 Census. It contains demographic information about each Zip Code in New York for 2010. We insert racial demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dem_path = os.getcwd() + '/2010/'\n",
    "demFiles = glob.glob(dem_path + \"/*.csv\")\n",
    "df_2010 = pd.read_csv(demFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add Overall Population Data\n",
    "df_2010 = df_2010[(df_2010.index != 0) & (df_2010['HD02_S32'] != '(X)')]\n",
    "\n",
    "pop = df_2010.apply(lambda row: (int(row['GEO.id2']),int(row['HD01_S01'])), axis =1 ).values\n",
    "pop = dict(pop)\n",
    "clean_df.loc[clean_df.YEAR == 2010, 'Population'] = clean_df.apply(lambda r: pop[r['ZIP CODE']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Add White Population\n",
    "pop = df_2010.apply(lambda row: (int(row['GEO.id2']),float(row['HD02_S03'])), axis =1 ).values\n",
    "pop = dict(pop)\n",
    "clean_df.loc[clean_df.YEAR == 2010, 'White'] = clean_df.apply(lambda r: pop[r['ZIP CODE']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add Black Population\n",
    "pop = df_2010.apply(lambda row: (int(row['GEO.id2']),float(row['HD02_S04'])), axis =1 ).values\n",
    "pop = dict(pop)                        \n",
    "clean_df.loc[clean_df.YEAR == 2010, 'Black'] = clean_df.apply(lambda r: pop[r['ZIP CODE']], axis = 1)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add Asain Population\n",
    "pop = df_2010.apply(lambda row: (int(row['GEO.id2']),float(row['HD02_S10'])), axis =1 ).values\n",
    "pop = dict(pop)\n",
    "clean_df.loc[clean_df.YEAR == 2010, 'Asian'] = clean_df.apply(lambda r: pop[r['ZIP CODE']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add Hispanic\n",
    "pop = df_2010.apply(lambda row: (int(row['GEO.id2']),float(row['HD02_S30'])), axis =1 ).values\n",
    "pop = dict(pop)\n",
    "clean_df.loc[clean_df.YEAR == 2010, 'Hispanic'] = clean_df.apply(lambda r: pop[r['ZIP CODE']], axis = 1)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Insert Employment Data\n",
    "<a id = '4.5'></a>\n",
    "\n",
    "We now add data regarding the labor force for each zip codes. This information comes from ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/jobs/'\n",
    "jobFiles = glob.glob(path + \"*jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_year = {}\n",
    "for y in range(2011,2015):\n",
    "    job_year[y] = {}\n",
    "\n",
    "ctg = ['Labor Force','Employed','Unemployed']\n",
    "\n",
    "for f in jobFiles:\n",
    "    with open(f, 'rU') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        print(\"File: \"+f)\n",
    "        \n",
    "        jobs_df = {}\n",
    "        for c in ctg:\n",
    "            jobs_df[c] = {}\n",
    "\n",
    "        for _ in range(0,3):\n",
    "            next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            items = row[1]\n",
    "            lst = items.split(',')\n",
    "\n",
    "            if int(lst[0]) in zc.keys():\n",
    "                \n",
    "                try: \n",
    "                    lf = int(lst[3])\n",
    "                except:\n",
    "                    if lst[3] == '-':\n",
    "                        lf = 0.\n",
    "                    else:\n",
    "                        lf = float(lst[3])\n",
    "                jobs_df['Labor Force'][int(lst[0])] = lf\n",
    "\n",
    "                try: \n",
    "                    em = int(lst[5])\n",
    "                except:\n",
    "                    if lst[5] == '-':\n",
    "                        em = 0.\n",
    "                    else:\n",
    "                        em = float(lst[5])\n",
    "                jobs_df['Employed'][int(lst[0])] = em\n",
    "\n",
    "                try:\n",
    "                    un = int(lst[7])\n",
    "                except:\n",
    "                    if lst[7] == '-':\n",
    "                        un = 0.\n",
    "                    else:\n",
    "                        un = float(lst[7])\n",
    "                jobs_df['Unemployed'][int(lst[0])] = un  \n",
    "\n",
    "        curr_y = int(f[-12:-8])\n",
    "        print(curr_y)\n",
    "        job_year[curr_y] = jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_dict = {}\n",
    "for job in ctg:\n",
    "    job_dict[job] = []\n",
    "    \n",
    "for row in clean_df.iterrows():\n",
    "    z_c = row[1]['ZIP CODE']\n",
    "    yr = row[1]['YEAR']\n",
    "    \n",
    "    if yr < 2011:\n",
    "        for k in job_dict.keys():\n",
    "            job_dict[k].append(0)\n",
    "        continue\n",
    "    \n",
    "    if yr == 2014: yr = 2013\n",
    "        \n",
    "    job_curr_yr = job_year[yr]\n",
    "    \n",
    "    for job in job_dict.keys():\n",
    "    \n",
    "        job_dict[job].append(job_curr_yr[job][int(z_c)])\n",
    "        \n",
    "            \n",
    "\n",
    "for k, v in job_dict.items():\n",
    "    clean_df[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Infomation about the Adjacent Zip Codes\n",
    "<a id = '4.6'></a>\n",
    "\n",
    "We computed which neighborhood each zip code resided. The underlying assumption here is that nearby zip codes impact home sales. Each neighbor zip code was given a dummy variable. This code is commented out because during our analysis we worried about the curse of dimensionality and problems associated with data in high-dimensional spaces. Future research should incorporate data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "url = 'https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "table = soup.find('table')\n",
    "neighbor=[i.text for i in table.find_all(attrs={\"headers\": \"header3\"})]\n",
    "\n",
    "neighbor2 = [i.split(',') for i in neighbor]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def make_numbers(l):\n",
    "    return [int(i) for i in l]\n",
    "neighbors = [make_numbers(l) for l in neighbor2]\n",
    "\n",
    "#This list forgot the Zip Code 10069. Hard code it into the correct neighborhood\n",
    "neighbors[26].append(10069)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "zipdic = {}\n",
    "for i in neighbors:\n",
    "    for k in i:\n",
    "        zipdic[k] = i\n",
    "zipdic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean_df['adjzips'] = clean_df['ZIP CODE'].apply(lambda r: zipdic[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "zips = set() #Ensures uniqueness\n",
    "for z in clean_df.adjzips:\n",
    "    zips.update(g for g in z)\n",
    "\n",
    "for zippy in zips:\n",
    "    clean_df[zippy] = [zippy in zips for zips in clean_df.adjzips]\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Augment DataFrame with Additional Information  \n",
    "<a id = '4.7'></a>\n",
    "\n",
    "We insert features from data already in the DataFrame. This includes a features for Month and Month Year of when the home was sold. Most important, we create a dummy variable for which fiscal quarter the home was solid in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['MONTH'] = clean_df['SALE DATE'].apply(lambda r: r.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['MONTH/YEAR'] = clean_df['SALE DATE'].apply(lambda r: (r.month,r.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we create a dummy variable for which quarter the home was sold\n",
    "def convert_to_quarter(month):\n",
    "    groups = [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]\n",
    "    quarters = [\"Q%s\" %i for i in range(1,5)]\n",
    "    for index, k in enumerate(groups):\n",
    "        if month in k: \n",
    "            return quarters[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df['qs'] = clean_df['MONTH'].apply(convert_to_quarter)\n",
    "dumbos = pd.get_dummies(clean_df['qs'])\n",
    "df2 = pd.concat([clean_df, dumbos], axis = 1)\n",
    "df2.drop(['Q1', 'qs'], inplace=True, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Insert the Street Name as a Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add = '2329 WASHINGTON AVENUE                   '\n",
    "add2= '306 EAST MOSHOLU PARKWAY, 3J  '\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "# This function takes in an address \n",
    "def get_street(address):\n",
    "    #strip spaces from beginning and end\n",
    "    addr = address.strip(' ')\n",
    "    \n",
    "    # separate from commas (one edge case)\n",
    "    head, sep, tail = addr.partition(',')   \n",
    "    words = head.split(' ')\n",
    "    \n",
    "    if len(addr) != 1:\n",
    "        length = len(words)\n",
    "        street = ''       \n",
    "        #edge cases (BATHGATE is a bitcomplicated)\n",
    "        if length == 2:\n",
    "            if hasNumbers(words):\n",
    "                street += words[1]\n",
    "            else:\n",
    "                street += words[0] + ' ' + words [1]\n",
    "        else:\n",
    "            for x in xrange(1,length):\n",
    "                street += words[x]\n",
    "                if x != length - 1:\n",
    "                    street += ' '\n",
    "        return street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test edge cases on complicated address\n",
    "get_street(add2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Adjacent Zip Code, our final anaylsis omitted the use of street names, from fear of having too many dummy variables. With more time, we would have clustered them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this should remain commented out\n",
    "#df2['STEET'] = df2['ADDRESS'].apply(lambda r: get_street(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dummy Variable for Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zips = list(set(df2['ZIP CODE'].values))\n",
    "just_dummies = pd.get_dummies(df2['ZIP CODE'])\n",
    "df3 = pd.concat([df2, just_dummies], axis=1)\n",
    "df3.drop([11694], inplace=True, axis=1) #drop one of the dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A Little Housing Cleaning/Visualization Before Analysis\n",
    "<a id = '5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df3['LAND SQUARE FEET']\n",
    "del df3['Street Cafes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column `YEAR BUILT`, missing values were registered as 0. We replace these 0's with the mean Year Built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3['YEAR BUILT'].replace(to_replace=0, value=df3['YEAR BUILT'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are missing vital information about 2010 from the American Community Survey. We have decided to omit 2010 from the analysis. Therefore, we will be training/testing on years 2011, 2012, 2013 and 2014 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle = df3[~df3['YEAR'].isin([2007,2008,2009,2010])]\n",
    "pickle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the final DataFrame for the other notebooks to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pickle.to_pickle('cleandf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
